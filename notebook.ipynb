{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b02e9b0",
   "metadata": {},
   "source": [
    "# Setting Up the Environment\n",
    "This section imports necessary libraries and sets up the environment for the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f1dd874",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3578eb47",
   "metadata": {},
   "source": [
    "# Connecting to Qdrant\n",
    "This section establishes a connection to the Qdrant vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed658fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import qdrant_client\n",
    "\n",
    "collection_name=\"chat_with_docs\"\n",
    "\n",
    "client = qdrant_client.QdrantClient(\n",
    "    host=\"localhost\",\n",
    "    port=6333\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972e8b6e",
   "metadata": {},
   "source": [
    "# Instrumentation Setup\n",
    "This section sets up instrumentation for tracing and monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd705230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî≠ OpenTelemetry Tracing Details üî≠\n",
      "|  Phoenix Project: default\n",
      "|  Span Processor: SimpleSpanProcessor\n",
      "|  Collector Endpoint: localhost:4317\n",
      "|  Transport: gRPC\n",
      "|  Transport Headers: {'user-agent': '****'}\n",
      "|  \n",
      "|  Using a default SpanProcessor. `add_span_processor` will overwrite this default.\n",
      "|  \n",
      "|  ‚ö†Ô∏è WARNING: It is strongly advised to use a BatchSpanProcessor in production environments.\n",
      "|  \n",
      "|  `register` has set this TracerProvider as the global OpenTelemetry default.\n",
      "|  To disable this behavior, call `register` with `set_global_tracer_provider=False`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from openinference.instrumentation.llama_index import LlamaIndexInstrumentor\n",
    "from phoenix.otel import register\n",
    "\n",
    "tracer_provider = register()\n",
    "LlamaIndexInstrumentor().instrument(tracer_provider=tracer_provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234b2044",
   "metadata": {},
   "source": [
    "# Loading Documents\n",
    "This section loads documents from the specified directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4363deb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "input_dir_path = './docs'\n",
    "\n",
    "loader = SimpleDirectoryReader(\n",
    "            input_dir = input_dir_path,\n",
    "            required_exts=[\".pdf\"],\n",
    "            recursive=True\n",
    "        )\n",
    "docs = loader.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d1b07a",
   "metadata": {},
   "source": [
    "# Checking Loaded Documents\n",
    "This section checks the type and number of loaded documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4096dacb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 63)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(docs), len(docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e75700",
   "metadata": {},
   "source": [
    "# Creating a Vector Store Index\n",
    "This section defines a function to create a vector store index using Qdrant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac428e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.core import VectorStoreIndex, ServiceContext, StorageContext\n",
    "\n",
    "def create_index(documents):\n",
    "\n",
    "    vector_store = QdrantVectorStore(client=client,\n",
    "                                     collection_name=collection_name)\n",
    "    \n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "    \n",
    "    index = VectorStoreIndex.from_documents(documents,\n",
    "                                            storage_context=storage_context)\n",
    "    \n",
    "    return index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5296eb",
   "metadata": {},
   "source": [
    "# Setting Up Embedding Model\n",
    "This section sets up the embedding model for the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2ee42e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1746540167.877371   46210 chttp2_transport.cc:1201] ipv6:%5B::1%5D:4317: Got goaway [11] err=UNAVAILABLE:GOAWAY received; Error code: 11; Debug Text: ping_timeout {grpc_status:14, http2_error:11, created_time:\"2025-05-06T19:32:47.875857944+05:30\"}\n",
      "Transient error StatusCode.UNAVAILABLE encountered while exporting traces to localhost:4317, retrying in 1s.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-large-en-v1.5\",\n",
    "                                   trust_remote_code=True)\n",
    "\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "index = create_index(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1c0d80",
   "metadata": {},
   "source": [
    "# Configuring LLM\n",
    "This section configures the language model for the query engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5bf02cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.groq import Groq\n",
    "from llama_index.core.settings import Settings\n",
    "\n",
    "llm = Groq(model=\"gemma2-9b-it\", request_timeout=120.0)\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f465d3c2",
   "metadata": {},
   "source": [
    "# Defining a Prompt Template\n",
    "This section defines a custom prompt template for the query engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05ec3757",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "template = \"\"\"Context information is below:\n",
    "              ---------------------\n",
    "              {context_str}\n",
    "              ---------------------\n",
    "              Given the context information above I want you to think\n",
    "              step by step to answer the query in a crisp manner,\n",
    "              incase you don't know the answer say 'I don't know!'\n",
    "            \n",
    "              Query: {query_str}\n",
    "        \n",
    "              Answer:\"\"\"\n",
    "\n",
    "qa_prompt_tmpl = PromptTemplate(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30005276",
   "metadata": {},
   "source": [
    "# Setting Up Reranking\n",
    "This section sets up a reranking mechanism for query results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba0b3e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "\n",
    "rerank = SentenceTransformerRerank(\n",
    "    model=\"cross-encoder/ms-marco-MiniLM-L-2-v2\", \n",
    "    top_n=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f89cc5d",
   "metadata": {},
   "source": [
    "# Querying the Engine\n",
    "This section demonstrates how to query the engine with a custom prompt and reranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6c1e675",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(similarity_top_k=10,\n",
    "                                     node_postprocessors=[rerank])\n",
    "\n",
    "query_engine.update_prompts(\n",
    "    {\"response_synthesizer:text_qa_template\": qa_prompt_tmpl}\n",
    ")\n",
    "\n",
    "response = query_engine.query(\"What exactly is ReSpAct?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5e207a",
   "metadata": {},
   "source": [
    "# Displaying the Response\n",
    "This section displays the response from the query engine in a markdown format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5345b15a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "ReSpAct is a task-oriented dialogue agent that leverages user interaction to enhance its decision-making capabilities. \n",
       "\n",
       "Here's a breakdown:\n",
       "\n",
       "* **ReSpAct stands for \"Reasoning and SpAct\"**:  It combines reasoning abilities with interactive strategies.\n",
       "* **Key Feature: User Interaction**: Unlike purely reasoning-based agents, ReSpAct actively seeks user input and guidance during task execution. This allows it to clarify ambiguities, handle unexpected situations, and learn from user feedback.\n",
       "* **Benefits**: This interactive approach leads to:\n",
       "    * **Improved Performance**: ReSpAct often outperforms purely reasoning-based agents (like ReAct) in complex tasks.\n",
       "    * **Robustness**: It can adapt to unfamiliar scenarios and handle user input that might be incomplete or even contradictory.\n",
       "    * **Efficiency**: By leveraging user knowledge, ReSpAct can sometimes complete tasks faster and with fewer trial-and-error attempts.\n",
       "\n",
       "* **How it Works**: ReSpAct uses a language model (like GPT-4o or LLaMA) as its core. It's trained on datasets that include examples of human-agent interactions in various task-oriented domains. During task execution, ReSpAct generates a sequence of actions, thoughts, and dialogue acts. When it encounters uncertainty or needs clarification, it interacts with the user, seeking information or guidance.\n",
       "\n",
       "\n",
       "Let me know if you have any more questions about ReSpAct!\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(str(response)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec42da3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8069fa0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
