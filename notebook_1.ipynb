{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b02e9b0",
   "metadata": {},
   "source": [
    "# Setting Up the Environment\n",
    "This section imports necessary libraries and sets up the environment for the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f1dd874",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3578eb47",
   "metadata": {},
   "source": [
    "# Connecting to Qdrant\n",
    "This section establishes a connection to the Qdrant vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed658fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import qdrant_client\n",
    "\n",
    "collection_name=\"chat_with_docs_v2\"\n",
    "\n",
    "client = qdrant_client.QdrantClient(\n",
    "    host=\"localhost\",\n",
    "    port=6333\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972e8b6e",
   "metadata": {},
   "source": [
    "# Instrumentation Setup\n",
    "This section sets up instrumentation for tracing and monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd705230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî≠ OpenTelemetry Tracing Details üî≠\n",
      "|  Phoenix Project: default\n",
      "|  Span Processor: SimpleSpanProcessor\n",
      "|  Collector Endpoint: localhost:4317\n",
      "|  Transport: gRPC\n",
      "|  Transport Headers: {'user-agent': '****'}\n",
      "|  \n",
      "|  Using a default SpanProcessor. `add_span_processor` will overwrite this default.\n",
      "|  \n",
      "|  ‚ö†Ô∏è WARNING: It is strongly advised to use a BatchSpanProcessor in production environments.\n",
      "|  \n",
      "|  `register` has set this TracerProvider as the global OpenTelemetry default.\n",
      "|  To disable this behavior, call `register` with `set_global_tracer_provider=False`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from openinference.instrumentation.llama_index import LlamaIndexInstrumentor\n",
    "from phoenix.otel import register\n",
    "\n",
    "tracer_provider = register()\n",
    "LlamaIndexInstrumentor().instrument(tracer_provider=tracer_provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234b2044",
   "metadata": {},
   "source": [
    "# Loading Documents\n",
    "This section loads documents from the specified directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4363deb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "input_dir_path = './docs'\n",
    "\n",
    "loader = SimpleDirectoryReader(\n",
    "            input_dir = input_dir_path,\n",
    "            required_exts=[\".pdf\"],\n",
    "            recursive=True\n",
    "        )\n",
    "docs = loader.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d1b07a",
   "metadata": {},
   "source": [
    "# Checking Loaded Documents\n",
    "This section checks the type and number of loaded documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4096dacb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 12)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(docs), len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e75700",
   "metadata": {},
   "source": [
    "# Creating a Vector Store Index\n",
    "This section defines a function to create a vector store index using Qdrant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac428e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.core import VectorStoreIndex, ServiceContext, StorageContext\n",
    "\n",
    "def create_index(documents):\n",
    "\n",
    "    vector_store = QdrantVectorStore(client=client,\n",
    "                                     collection_name=collection_name)\n",
    "    \n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "    \n",
    "    index = VectorStoreIndex.from_documents(documents,\n",
    "                                            storage_context=storage_context)\n",
    "    \n",
    "    return index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5296eb",
   "metadata": {},
   "source": [
    "# Setting Up Embedding Model\n",
    "This section sets up the embedding model for the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ee42e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2a4cef2bb344539b76d79a09b815517",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c6a971fcf164df58714738cd754d6cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af2ab86c528a4cdf9ffc3b29200cd7b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/94.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74ae43d0e7fe413ba11e206c82ff7749",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48daad697b674ed589022f857074e8af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/779 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11d19565054b406e914048318c2580bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73314a39457f4f6285d48aa2293ac528",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8833f959838c48988391f7a809521506",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45231bdb0d654718969551a22a7b0bae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd4a5073ccbf4952933d2eeacb59f254",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aa33e32502547aca4b7b5d8744eb5b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/191 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1747466909.776390   37722 chttp2_transport.cc:1201] ipv6:%5B::1%5D:4317: Got goaway [11] err=UNAVAILABLE:GOAWAY received; Error code: 11; Debug Text: ping_timeout {grpc_status:14, http2_error:11, created_time:\"2025-05-17T12:58:29.773682233+05:30\"}\n",
      "Transient error StatusCode.UNAVAILABLE encountered while exporting traces to localhost:4317, retrying in 1s.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-large-en-v1.5\",\n",
    "                                   trust_remote_code=True)\n",
    "\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "index = create_index(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a54a6e3",
   "metadata": {},
   "source": [
    "### Do Not Use Gemini/GCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cbcdba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.google_genai import GoogleGenAIEmbedding\n",
    "from llama_index.core import Settings\n",
    "from google.auth import default\n",
    "\n",
    "credentials, _ = default()\n",
    "\n",
    "project = os.getenv(\"GOOGLE_CLOUD_PROJECT_ID\")\n",
    "location = os.getenv(\"GOOGLE_CLOUD_LOCATION\")\n",
    "\n",
    "\n",
    "embed_model = GoogleGenAIEmbedding(\n",
    "    model_name=\"text-embedding-004\",\n",
    "    embed_batch_size=100,\n",
    "    vertexai_config={\n",
    "        \"project\": project,\n",
    "        \"location\": 'us-central1',\n",
    "        \"credentials\": credentials\n",
    "    }\n",
    ")\n",
    "\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "index = create_index(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1c0d80",
   "metadata": {},
   "source": [
    "# Configuring LLM\n",
    "This section configures the language model for the query engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5bf02cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: Hello! I'm doing well, thank you for asking. I'm ready to assist you with whatever you need. How can I help you today?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example for Groq (commented out for now)\n",
    "# from llama_index.llms.groq import Groq\n",
    "# llm = Groq(model=\"gemma2-9b-it\", request_timeout=120.0)\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "from google.auth import default\n",
    "from llama_index.llms.vertex import Vertex\n",
    "from llama_index.core.settings import Settings\n",
    "from llama_index.core.base.llms.types import ChatMessage\n",
    "\n",
    "# Suppress Vertex deprecation warnings (switch to GoogleGenAI soon)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# Google Cloud default credentials\n",
    "credentials, _ = default()\n",
    "\n",
    "# Initialize Vertex LLM\n",
    "llm = Vertex(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0.7,\n",
    "    credentials=credentials,\n",
    "    project=os.getenv(\"GOOGLE_CLOUD_PROJECT_ID\"),\n",
    "    location=os.getenv(\"GOOGLE_CLOUD_PROJECT_REGION\"),\n",
    ")\n",
    "\n",
    "# Set as default LLM for LlamaIndex\n",
    "Settings.llm = llm\n",
    "\n",
    "# Example chat call\n",
    "response = llm.chat([\n",
    "    ChatMessage(role=\"user\", content=\"Hello there Gemini! How are you doing today?\")\n",
    "])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f465d3c2",
   "metadata": {},
   "source": [
    "# Defining a Prompt Template\n",
    "This section defines a custom prompt template for the query engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05ec3757",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "template = \"\"\"Context information is below:\n",
    "              ---------------------\n",
    "              {context_str}\n",
    "              ---------------------\n",
    "              Given the context information above I want you to think\n",
    "              step by step to answer the query in a crisp manner,\n",
    "              incase you don't know the answer say 'I don't know!'\n",
    "            \n",
    "              Query: {query_str}\n",
    "        \n",
    "              Answer:\"\"\"\n",
    "\n",
    "qa_prompt_tmpl = PromptTemplate(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30005276",
   "metadata": {},
   "source": [
    "# Setting Up Reranking\n",
    "This section sets up a reranking mechanism for query results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba0b3e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "\n",
    "rerank = SentenceTransformerRerank(\n",
    "    model=\"cross-encoder/ms-marco-MiniLM-L-2-v2\", \n",
    "    top_n=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f89cc5d",
   "metadata": {},
   "source": [
    "# Querying the Engine\n",
    "This section demonstrates how to query the engine with a custom prompt and reranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6c1e675",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(similarity_top_k=10,\n",
    "                                     node_postprocessors=[rerank])\n",
    "\n",
    "query_engine.update_prompts(\n",
    "    {\"response_synthesizer:text_qa_template\": qa_prompt_tmpl}\n",
    ")\n",
    "\n",
    "response = query_engine.query(\"What exactly is Activ One? and tell me all its benefits in detail\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5e207a",
   "metadata": {},
   "source": [
    "# Displaying the Response\n",
    "This section displays the response from the query engine in a markdown format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5345b15a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here's a breakdown of what Activ One is and its benefits, based on the provided context:\n",
       "\n",
       "**What Activ One Is:**\n",
       "\n",
       "*   Activ One is a health insurance plan offered by Aditya Birla Health Insurance Co. Limited.\n",
       "*   It's designed to protect you from rising medical costs, acting as an \"inflation-proof\" plan.\n",
       "\n",
       "**Benefits of Activ One (in detail):**\n",
       "\n",
       "*   **Unlimited Coverage:** Aims to provide limitless coverage. This means no sub-limits, and base benefits are covered up to the Sum Insured.\n",
       "*   **Increasing Sum Insured:** The \"Super Credit\" feature inflates the Sum Insured up to 6 times the original amount by the 6th year of the policy, regardless of claims. Optional feature Activ One NXT, 3X from 2nd renewal and onwards for Activ One VYTL\n",
       "*   **Wide Range of Sum Insured:** Offers a range of Sum Insured options, from INR 5 lacs to INR 2 crores.\n",
       "*   **Zone-Based Premium:** Premium is determined by the city of residence.\n",
       "*   **No Maximum Entry Age:** There is no upper age limit to enroll in the plan.\n",
       "*   **Age-Banded Premium:** Premium increases only after certain age bands, not every year.\n",
       "*   **Covers Live-in Partners:** Now covers legally married spouse or live-in partner (same or opposite sex)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(str(response)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec42da3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8069fa0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
